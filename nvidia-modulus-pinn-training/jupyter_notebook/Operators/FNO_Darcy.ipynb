{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b13558",
   "metadata": {},
   "source": [
    "# Data-driven Darcy Flow Using FNO and its Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de577c",
   "metadata": {},
   "source": [
    "In this notebook we will cover some theory behind the Fourier Neural Operators and some of their variants. We will then apply these architectures on a Darcy flow field prediction problem in a data-driven fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400f9f8",
   "metadata": {},
   "source": [
    "### Learning Outcomes\n",
    "1. A brief theory behind the Fourier Neural Operators (FNO), Adaptive Fourier Neural Operators (AFNO) and Physics-informed Neural Operators (PINO) \n",
    "2. How to set up and train the three models in Modulus\n",
    "3. Differences between the three variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276db56",
   "metadata": {},
   "source": [
    "## Fourier Neural Operators (FNO)\n",
    "\n",
    "Fourier neural operator (FNO) is a data-driven architecture which can be used to parameterize solutions for a distribution of PDE solutions. The key feature of FNO is the spectral convolutions: operations that place the integral kernel in Fourier space. The spectral convolution (Fourier integral operator) is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "(\\mathcal{K}(\\mathbf{w})\\phi)(x) = \\mathcal{F}^{-1}(R_{\\mathbf{W}}\\cdot \\left(\\mathcal{F}\\right)\\phi)(x), \\quad \\forall x \\in D\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ are the forward and inverse Fourier transforms, respectively.\n",
    "$R_{\\mathbf{w}}$ is the transformation which contains the learn-able parameters $\\mathbf{w}$. Note this operator is calculated\n",
    "over the entire *structured Euclidean* domain $D$ discretized with $n$ points.\n",
    "\n",
    "Fast Fourier Transform (FFT) is used to perform the Fourier transforms efficiently and the resulting transformation $R_{\\mathbf{w}}$ is just a finite size matrix of learn-able weights. Inside the spectral convolution, the Fourier coefficients are truncated to only the lower modes which allows explicit control over the dimensionality of the spectral space and linear operator.\n",
    "\n",
    "The FNO model is the composition of a fully-connected \"lifting\" layer, $L$ spectral convolutions with point-wise linear skip connections and a decoding point-wise fully-connected neural network at the end.\n",
    "\n",
    "\\begin{equation}\n",
    "u_{net}(\\Phi;\\theta) = \\mathcal{Q}\\circ \\sigma(W_{L} + \\mathcal{K}_{L}) \\circ ... \\circ \\sigma(W_{1} + \\mathcal{K}_{1})\\circ \\mathcal{P}(\\Phi), \\quad \\Phi=\\left\\{\\phi(x); \\forall x \\in D\\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "...in which $\\sigma(W_{i} + \\mathcal{K}_{i})$ is the spectral convolution layer $i$ with the point-wise linear transform $W_{i}$ and activation function $\\sigma(\\cdot)$. $\\mathcal{P}$ is the point-wise lifting network that projects the input into a higher-dimensional latent space, $\\mathcal{P}: \\mathbb{R}^{d_in} \\rightarrow \\mathbb{R}^{k}$.\n",
    "\n",
    "Similarly, $\\mathcal{Q}$ is the point-wise fully-connected decoding network, $\\mathcal{P}: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d_out}$. Since all fully-connected components of FNO are point-wise operations, the model is invariant to the dimensionality of the input.\n",
    "\n",
    "<img src=\"fno_darcy.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "For more details, please refer the [Modulus User Documentation](https://docs.nvidia.com/deeplearning/modulus/user_guide/theory/architectures.html#fourier-neural-operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5212d",
   "metadata": {},
   "source": [
    "## Adaptive Fourier Neural Operators (AFNO)\n",
    "\n",
    "In contrast with the Fourier Neural Operator which has a convolutional architecture, the AFNO leverages contemporary transformer architectures in the computer vision domain. Vision transformers have delivered tremendous success in computer vision. This is primarily due to effective self-attention mechanisms. To cope with this challenge, Guibas et al. proposed [Adaptive Fourier Neural Operator (AFNO)](https://www.researchgate.net/publication/356601975_Adaptive_Fourier_Neural_Operators_Efficient_Token_Mixers_for_Transformers) as an efficient attention mechanism in the Fourier Domain. AFNO is based on principled foundation of operator learning which allows us to frame attention as a continuous global convolution efficiently in the Fourier domain. To handle challenges in vision such as discontinuities in images and high-resolution inputs, AFNO proposes principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. \n",
    "\n",
    "The AFNO model typically includes the following steps:\n",
    "1. Dividing the input image into a regular grid with $h \\times w$ equal sized patches of size $p\\times p$.\n",
    "2. Embed the patch into a token of size $d$, the embedding dimension resulting in a token tensor ($X_{h\\times w \\times d}$) of size $h \\times w \\times d$. \n",
    "3. Pass the token through multiple layers of transformer architecture performing spatial and channel mixing. \n",
    "4. At the end of all the transformer layers, convert the feature tensor back to image space using a linear decoder.\n",
    "\n",
    "For each layer in the Step 3, the AFNO architecture implements the following operations: \n",
    "\n",
    "The token tensor is first transformed to the Fourier domain with\n",
    "\n",
    "\\begin{equation}\n",
    "z_{m,n} = [\\mathrm{DFT}(X)]_{m,n},\n",
    "\\end{equation}\n",
    "\n",
    "where $m,n$ is the index the patch location and DFT denotes a 2D discrete Fourier transform. The model then applies token weighting in the Fourier domain and promotes sparsity with a Soft-Thresholding and Shrinkage operation as\n",
    "\n",
    "\\begin{equation} \n",
    "\\tilde{z}_{m,n} = S_{\\lambda} ( \\mathrm{MLP}(z_{m,n})),\n",
    "\\end{equation}\n",
    "\n",
    "where $S_{\\lambda}(x) = \\mathrm{sign}(x) \\max(|x| - \\lambda, 0)$ with the sparsity controlling parameter $\\lambda$, and $\\mathrm{MLP(\\cdot)}$ is a 2-layer multi-layer perceptron with block-diagonal weight matrices which are shared across all patches. \n",
    "\n",
    "The last operation in an ANFO layer is an inverse Fourier to transform back to the patch domain and add a residual connection as\n",
    "\n",
    "\\begin{equation}\n",
    "y_{m,n} = [\\mathrm{IDFT}(\\tilde{Z})]_{m,n} + X_{m,n}.\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"afno_darcy.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "The complete mathematical description of the AFNO model is beyond the scope of this material, and we refer you to [Modulus User Documentation](https://docs.nvidia.com/deeplearning/modulus/user_guide/theory/architectures.html#adaptive-fourier-neural-operator) for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e39ffd",
   "metadata": {},
   "source": [
    "## Physics-informed Neural Operators\n",
    "\n",
    "The Physics-Informed Neural Operator (PINO) was introduced by Li et al. in this [paper](https://arxiv.org/abs/2111.03794). The PINO approach for surrogate modeling of PDE systems effectively combines the data-informed supervised learning framework of the FNO with the physics-informed learning framework. The PINO incorporates a PDE loss $\\mathcal{L}_{pde}$ to the Fourier Neural Operator. This reduces the amount of data required to train a surrogate model, since the PDE loss constrains the solution space. \n",
    "The PDE loss also enforces physical constraints on the solution computed by a surrogate ML model, making it an attractive option as a verifiable, accurate and interpretable ML surrogate modeling tool.\n",
    "\n",
    "To explain the key concepts behind PINO, consider a stationary PDE system. Following the notation used in the [paper](https://arxiv.org/abs/2111.03794), we consider a PDE represented by,\n",
    "\n",
    "\\begin{equation} \n",
    "\\mathcal{P}(u, a) = 0 , \\text{ in } D \\subset \\mathbb{R}^d,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "u = g ,  \\text{ in } \\partial D.\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\mathcal{P}$ is a Partial Differential Operator, $a$ are the coefficients/parameters and $u$ is the PDE solution.\n",
    "\n",
    "In the FNO framework, the surrogate ML model is given by the solution operator $\\mathcal{G}^\\dagger_{\\theta}$, which maps any given coefficient in the coefficient space $a$ to the solution $u$. The FNO is trained in a supervised fashion using training data in the form of input/output pairs $\\lbrace a_j, u_j \\rbrace_{j = 1}^N$. The training loss for the FNO is given by summing the data loss, $\\mathcal{L}_{data}(\\mathcal{G}_\\theta) = \\lVert u - \\mathcal{G}_\\theta(a)  \\rVert^2$ over all training pairs $\\lbrace a_i, u_i,  \\rbrace_{i=1}^N$.\n",
    "\n",
    "In the PINO framework, the solution operator is optimized with an additional PDE loss given by $\\mathcal{L}_{pde}(a, \\mathcal{G}_{\\theta}(a))$ computed over i.i.d. samples $a_j$ from an appropriate supported distribution in parameter/coefficient space.\n",
    "\n",
    "In general, the PDE loss involves computing the PDE operator which in turn involves computing the partial derivatives of the Fourier Neural Operator ansatz. In general, this is nontrivial. The key set of innovations in the PINO are the various ways to compute the partial derivatives of the operator ansatz, viz:\n",
    "\n",
    "1. Numerical differentiation using a Finite-Difference Method (FDM).\n",
    "2. Numerical differentiation computed via spectral derivative. \n",
    "3. Hybrid differentiation based on a combination of first-order \"exact\" derivatives and second-order FDM derivatives. \n",
    "\n",
    "For more details, please refer the [Modulus User Documentation](https://docs.nvidia.com/deeplearning/modulus/user_guide/theory/architectures.html#physics-informed-neural-operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37c1be",
   "metadata": {},
   "source": [
    "Now, with this theoretical background, let's solve the Darcy flow problem using these three approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7e903",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "The Darcy PDE is a second order, elliptic PDE with the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "-\\nabla \\cdot \\left(k(\\textbf{x})\\nabla u(\\textbf{x})\\right) = f(\\textbf{x}), \\quad \\textbf{x} \\in D,\n",
    "\\end{equation}\n",
    "\n",
    "...in which $u(\\textbf{x})$ is the flow pressure, $k(\\textbf{x})$ is the permeability field and $f(\\cdot)$ is the\n",
    "forcing function. The Darcy flow can parameterize a variety of systems including flow through porous media, elastic materials \n",
    "and heat conduction. Here, we will define the domain as a 2D unit square  $D=\\left\\{x,y \\in (0,1)\\right\\}$ with the boundary condition $u(\\textbf{x})=0, \\textbf{x}\\in\\partial D$. Both the permeability and flow fields are discretized into a 2D matrix $\\textbf{K}, \\textbf{U} \\in \\mathbb{R}^{N \\times N}$.\n",
    "\n",
    "The goal of this problem is to develop a surrogate model that learns the mapping between a permeability field and the pressure field,\n",
    "$\\textbf{K} \\rightarrow \\textbf{U}$, for a distribution of permeability fields $\\textbf{K} \\sim p(\\textbf{K})$.\n",
    "In this problem, you are *not* learning just a single solution but rather a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f64fc",
   "metadata": {},
   "source": [
    "## Download the Required Data\n",
    "\n",
    "Before starting any coding, we need to make sure that we have both the training and the validation data. The training and validation datasets for this example can be found on the [Fourier Neural Operator GitHub page](https://github.com/zongyi-li/fourier_neural_operator). The script [`utilities.py`](../../source_code/darcy/utilities.py) is an automated script for downloading and converting this dataset. This requires the package [gdown](https://github.com/wkentaro/gdown) which can be easily installed through `pip install gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "import os\n",
    "os.chdir('../../source_code/darcy/')\n",
    "\n",
    "from utilities import download_FNO_dataset, load_FNO_dataset\n",
    "from modulus.dataset import HDF5GridDataset\n",
    "from modulus.key import Key\n",
    "\n",
    "download_FNO_dataset(\"Darcy_241\", outdir=\"datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eba3df",
   "metadata": {},
   "source": [
    "## Part 1: Solution Using FNO\n",
    "\n",
    "Let's see how to solve the problem using the FNOs. The complete problem setup can be found in the [`darcy_FNO_lazy.py`](../../source_code/darcy/darcy_FNO_lazy.py) script, but here for the sake of understanding, we will build the constraints and the case step by step. [`config_FNO.yaml`](../../source_code/darcy/conf/config_FNO.yaml) lists the configs required for the problem. Let's start by loading them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulus.hydra import to_yaml, to_absolute_path\n",
    "from modulus.hydra.utils import compose\n",
    "from modulus.hydra.config import ModulusConfig\n",
    "\n",
    "cfg = compose(config_path=\"../../source_code/darcy/conf\", config_name=\"config_FNO\")\n",
    "cfg.network_dir = 'outputs/darcy_FNO'\n",
    "print(to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b6adb",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "For this data-driven problem, the first step is to get the training data into Modulus. Prior to loading data, we can set any normalization value that we want to apply to the data. For this dataset, we calculated the scale and shift parameters for both the input permeability field and output pressure. Then, we set this normalization inside Modulus by providing the scale/shift to each key, `Key(name, scale=(shift, scale))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = [Key(\"coeff\", scale=(7.48360e00, 4.49996e00))]\n",
    "output_keys = [Key(\"sol\", scale=(5.74634e-03, 3.88433e-03))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54798ea9",
   "metadata": {},
   "source": [
    "There are two approaches for loading data. The first is to use eager loading where you immediately read the entire dataset into memory at one time. Alternatively, you can use lazy loading where the data is loaded on a per-example basis as the model needs it for training. The eager loading eliminates potential overhead from reading data from disc during training, however this cannot scale to large datasets. Lazy loading is used in this example for both the training and test datasets to demonstrate this utility for larger problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = to_absolute_path(\"datasets/Darcy_241/piececonst_r241_N1024_smooth1.hdf5\")\n",
    "test_path = to_absolute_path(\"datasets/Darcy_241/piececonst_r241_N1024_smooth2.hdf5\")\n",
    "\n",
    "# make datasets\n",
    "train_dataset = HDF5GridDataset(train_path, invar_keys=[\"coeff\"], outvar_keys=[\"sol\"], n_examples=1000)\n",
    "test_dataset = HDF5GridDataset(test_path, invar_keys=[\"coeff\"], outvar_keys=[\"sol\"], n_examples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ff16f",
   "metadata": {},
   "source": [
    "If you are interested in seeing how the eager loading works, please refer [`darcy_FNO.py`](../../source_code/darcy/darcy_FNO.py) script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9937a",
   "metadata": {},
   "source": [
    "### Initializing the Model and Domain, and Adding Constraints\n",
    "\n",
    "Initializing the model and domain follows the same steps as the other PINN models we saw earlier. \n",
    "\n",
    "For the physics-informed problems in Modulus, we typically need to define a geometry and constraints based on boundary conditions and governing equations. Here, the only constraint is a [`SupervisedGridConstraint`](https://docs.nvidia.com/deeplearning/modulus/api/modulus.domain.constraint.html#modulus.domain.constraint.discrete.SupervisedGridConstraint) which performs standard supervised training on grid data. This constraint supports the use of multiple workers, which are particularly important when using lazy loading. Let's see how this can be achieved in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f73be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulus.hydra import instantiate_arch\n",
    "from modulus.domain import Domain\n",
    "from modulus.domain.constraint import SupervisedGridConstraint\n",
    "\n",
    "# make list of nodes to unroll graph on\n",
    "model = instantiate_arch(\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    cfg=cfg.arch.fno,\n",
    ")\n",
    "nodes = model.make_nodes(name=\"FNO\", jit=cfg.jit)\n",
    "\n",
    "# make domain\n",
    "domain = Domain()\n",
    "\n",
    "# add constraints to domain\n",
    "supervised = SupervisedGridConstraint(\n",
    "    nodes=nodes,\n",
    "    dataset=train_dataset,\n",
    "    batch_size=cfg.batch_size.grid,\n",
    "    num_workers=4,  # number of parallel data loaders\n",
    ")\n",
    "domain.add_constraint(supervised, \"supervised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49226509",
   "metadata": {},
   "source": [
    "**Note:** Grid data refers to data that can be defined in a tensor like an image. Inside Modulus, this grid of data typically represents a spatial domain and should follow the standard dimensionality of `[batch, channel, xdim, ydim, zdim]` where channel is the dimensionality of your state variables. Both Fourier and convolutional models use grid-based data to efficiently learn and predict entire domains in one forward pass, which contrasts to the point-wise predictions of standard PINN approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfed22a",
   "metadata": {},
   "source": [
    "### Adding Data Validator\n",
    "\n",
    "We can then add the validation data to the domain using [`GridValidator`](https://docs.nvidia.com/deeplearning/modulus/api/modulus.domain.validator.html#modulus.domain.validator.discrete.GridValidator) which should be used when dealing with structured data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulus.domain.validator import GridValidator\n",
    "from modulus.utils.io.plotter import GridValidatorPlotter\n",
    "\n",
    "# add validator\n",
    "val = GridValidator(\n",
    "    nodes,\n",
    "    dataset=test_dataset,\n",
    "    batch_size=cfg.batch_size.validation,\n",
    "    plotter=GridValidatorPlotter(n_examples=5),\n",
    ")\n",
    "domain.add_validator(val, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3640f61",
   "metadata": {},
   "source": [
    "### Solver and Training the Model \n",
    "\n",
    "We can create a solver by using the domain we just created along with the other configurations that define the optimizer choices, settings using Modulus’ `Solver` class. The solver can then be executed using the `solve` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the logging work in the jupyter cells\n",
    "# Need to execute this only once!\n",
    "import logging\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687122ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from modulus.solver import Solver\n",
    "\n",
    "# optional set appropriate GPU in case of multi-GPU machine\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "# make solver\n",
    "slv = Solver(cfg, domain)\n",
    "\n",
    "# start solver\n",
    "slv.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072964db",
   "metadata": {},
   "source": [
    "### Results and Post-processing\n",
    "\n",
    "The checkpoint directory is saved based on the results recording frequency specified in the `rec_results_freq` parameter of its derivatives. The network directory folder contains several plots of different validation predictions. Several are shown below, and you can see that the model is able to accurately predict the pressure field for permeability fields it had not seen previously. These visualizations can also be viewed during the training using the Tensorboard. \n",
    "\n",
    "<figure>\n",
    "    <figcaption>FNO validation predictions. (Left to right) Input permeability, true pressure, predicted pressure, error.</figcaption>\n",
    "    <img src=\"fno_darcy_pred1.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "    <img src=\"fno_darcy_pred2.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "    <img src=\"fno_darcy_pred3.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae3541",
   "metadata": {},
   "source": [
    "## Part 2: Solution Using AFNO\n",
    "\n",
    "Let's see how to solve the problem using the AFNOs. The complete problem setup can be found in the [`darcy_AFNO.py`](../../source_code/darcy/darcy_AFNO.py) script, but here for the sake of understanding, we will build the constraints and the case step by step. [`config_AFNO.yaml`](../../source_code/darcy/conf/config_AFNO.yaml) lists the configs required for the problem. Let's start by loading them. The process of setting the AFNO training is very similar from a case setup standpoint hence we will only highlight the important differences wherever necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41caae5f",
   "metadata": {},
   "source": [
    "The AFNO is based on the ViT transformer architecture and requires tokenization of the inputs. Here each token is a patch of the image with a patch size defined in the configuration file through the parameter `patch_size`. The `embed_dim` parameter defines the size of the latent embedded features used inside the model for each patch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_path=\"../../source_code/darcy/conf\", config_name=\"config_AFNO\")\n",
    "cfg.network_dir = 'outputs/darcy_AFNO'\n",
    "print(to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5588d",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "Loading both the training and validation datasets into memory follows a similar process as seen in the FNO. The inputs for AFNO need to be perfectly divisible by the specified patch size (in this case, `patch_size=16`), which is not the case for this dataset. Therefore, we trim the input/output features such that they have appropriate dimensionality `241x241 -> 240x240`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulus.dataset import DictGridDataset\n",
    "\n",
    "# load training/ test data\n",
    "input_keys = [Key(\"coeff\", scale=(7.48360e00, 4.49996e00))]\n",
    "output_keys = [Key(\"sol\", scale=(5.74634e-03, 3.88433e-03))]\n",
    "\n",
    "invar_train, outvar_train = load_FNO_dataset(\n",
    "    \"datasets/Darcy_241/piececonst_r241_N1024_smooth1.hdf5\",\n",
    "    [k.name for k in input_keys],\n",
    "    [k.name for k in output_keys],\n",
    "    n_examples=1000,\n",
    ")\n",
    "invar_test, outvar_test = load_FNO_dataset(\n",
    "    \"datasets/Darcy_241/piececonst_r241_N1024_smooth2.hdf5\",\n",
    "    [k.name for k in input_keys],\n",
    "    [k.name for k in output_keys],\n",
    "    n_examples=100,\n",
    ")\n",
    "\n",
    "# get training image shape\n",
    "img_shape = [\n",
    "    next(iter(invar_train.values())).shape[-2],\n",
    "    next(iter(invar_train.values())).shape[-1],\n",
    "]\n",
    "\n",
    "# crop out some pixels so that img_shape is divisible by patch_size of AFNO\n",
    "img_shape = [s - s % cfg.arch.afno.patch_size for s in img_shape]\n",
    "print(f\"cropped img_shape: {img_shape}\")\n",
    "for d in (invar_train, outvar_train, invar_test, outvar_test):\n",
    "    for k in d:\n",
    "        d[k] = d[k][:, :, : img_shape[0], : img_shape[1]]\n",
    "        print(f\"{k}: {d[k].shape}\")\n",
    "\n",
    "# make datasets\n",
    "train_dataset = DictGridDataset(invar_train, outvar_train)\n",
    "test_dataset = DictGridDataset(invar_test, outvar_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf76649",
   "metadata": {},
   "source": [
    "### Initializing the Model and Domain, Adding Constraints and Validators.\n",
    "\n",
    "These steps are the same as what we saw in the case of FNOs. For AFNO, we calculate the size of the domain after loading the dataset. The domain needs to be defined in the AFNO model, which is provided with the inclusion of the keyword argument `img_shape` in the `instantiate_arch` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50112a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of nodes to unroll graph on\n",
    "model = instantiate_arch(\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    cfg=cfg.arch.afno,\n",
    "    img_shape=img_shape,\n",
    ")\n",
    "nodes = [model.make_node(name=\"AFNO\", jit=cfg.jit)]\n",
    "\n",
    "# make domain\n",
    "domain = Domain()\n",
    "\n",
    "# add constraints to domain\n",
    "supervised = SupervisedGridConstraint(\n",
    "    nodes=nodes,\n",
    "    dataset=train_dataset,\n",
    "    batch_size=cfg.batch_size.grid,\n",
    ")\n",
    "domain.add_constraint(supervised, \"supervised\")\n",
    "\n",
    "# add validator\n",
    "val = GridValidator(\n",
    "    nodes,\n",
    "    dataset=test_dataset,\n",
    "    batch_size=cfg.batch_size.validation,\n",
    "    plotter=GridValidatorPlotter(n_examples=5),\n",
    ")\n",
    "domain.add_validator(val, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb16d4",
   "metadata": {},
   "source": [
    "### Solver and Training the Model\n",
    "\n",
    "Once the domain and the configuration is set up, the `Solver` can be defined, and the training can be started as seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e16990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make solver\n",
    "slv = Solver(cfg, domain)\n",
    "\n",
    "# start solver\n",
    "slv.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299017c",
   "metadata": {},
   "source": [
    "### Results and Post-processing\n",
    "\n",
    "The checkpoint directory is saved based on the results recording frequency specified in the `rec_results_freq` parameter of its derivatives. The network directory folder contains several plots of the different validation predictions, some of which are shown below. \n",
    "\n",
    "<figure>\n",
    "    <figcaption>AFNO validation predictions. (Left to right) Input permeability, true pressure, predicted pressure, error.</figcaption>\n",
    "    <img src=\"afno_darcy_pred1.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "    <img src=\"afno_darcy_pred2.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "    <img src=\"afno_darcy_pred3.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "</figure>\n",
    "\n",
    "It is important to recognize that AFNO's strength lies in its ability to scale to a much larger model size and datasets than what is used in this notebook/example. While not illustrated here, this example demonstrates the fundamental implementation of data-driven training using the AFNO architecture in Modulus for you to extend to larger problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65200a1",
   "metadata": {},
   "source": [
    "## Part 3: Solution Using PINO\n",
    "\n",
    "The key difference between PINO and FNO is that PINO adds a physics-informed term to the loss function of FNO. As discussed further in the theory, the PINO loss function is described by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\mathcal{L}_{data} + \\mathcal{L}_{pde},\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{data} = \\lVert u - \\mathcal{G}_\\theta(a)  \\rVert^2 ,\n",
    "\\end{equation}\n",
    "\n",
    "...where $\\mathcal{G}_\\theta(a)$ is an FNO model with learnable parameters $\\theta$ and input field $a$, and \n",
    "$\\mathcal{L}_{pde}$ is an appropriate PDE loss. For the 2D Darcy problem this is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{pde} = \\lVert -\\nabla \\cdot \\left(k(\\textbf{x})\\nabla \\mathcal{G}_\\theta(a)(\\textbf{x})\\right) - f(\\textbf{x}) \\rVert^2 ,\n",
    "\\end{equation}\n",
    "\n",
    "…where $k(\\textbf{x})$ is the permeability field, $f(\\textbf{x})$ is the forcing function equal to 1 in this case, and $a=k$ in this case.\n",
    "\n",
    "Note that the PDE loss involves computing various partial derivatives of the FNO ansatz, $\\mathcal{G}_\\theta(a)$. \n",
    "In Modulus, three different methods for computing these are provided. These are based on the original PINO paper:\n",
    "\n",
    "1. Numerical differentiation computed via Finite-Difference Method (FDM)\n",
    "2. Numerical differentiation computed via spectral derivative\n",
    "3. Hybrid differentiation based on a combination of first-order \"exact\" derivatives and second-order FDM derivatives\n",
    "\n",
    "The first 2 approaches are the same as proposed in the original paper. The third approach is a modification of the \"exact\" approach proposed in the paper.\n",
    "This method is slower and more memory intensive than the numerical derivative approaches when computing second order derivatives\n",
    "because it requires the computation of a Hessian matrix. \n",
    "Instead, a \"hybrid\" approach is provided which offers a compromise by combining first-order \"exact\"(the exact method is not technically exact because it uses a combination of numerical spectral derivatives and exact differentiation, see original paper for more details) derivatives and second-order FDM derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318540c",
   "metadata": {},
   "source": [
    "Now considering the problem setup, it is largely the same as the FNO example, except that the PDE loss is defined and the FNO model is constrained using it. This process is described in detail when we define the PDE loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cba4e",
   "metadata": {},
   "source": [
    "Let's start by loading the configs for this problem. The configuration for this problem is similar to the FNO example, but importantly there is an extra parameter `custom.gradient_method` where the method for computing the gradients in the PDE loss is selected. This can be one of `fdm`, `fourier`, or `hybrid` corresponding to the three options above. The balance between the data and PDE terms in the loss function can also be controlled using the `loss.weights` parameter group. The [`config_PINO.yaml`](../../source_code/darcy/conf/config_PINO.yaml) is loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_path=\"../../source_code/darcy/conf\", config_name=\"config_PINO\")\n",
    "cfg.network_dir = 'outputs/darcy_PINO'\n",
    "print(to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a4942",
   "metadata": {},
   "source": [
    "### Define PDE Loss for Grid Data\n",
    "\n",
    "For this example, a custom PDE residual calculation is defined using the various approaches proposed above. The process of defining a custom PDE residual using SymPy and auto-diff was discussed in the [notebooks on PINNs](../diffusion_1d/Diffusion_Problem_Notebook.ipynb). For this problem, we will not be relying on standard auto-diff for calculating the derivatives, instead we want to explicitly define how the residual is calculated using a custom `torch.nn.Module` called `Darcy`. The purpose of this module is to compute and return the Darcy PDE residual given the input and output tensors of the FNO model, which is done via its `.forward(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict\n",
    "\n",
    "# import the custom ops\n",
    "from ops import dx, ddx\n",
    "\n",
    "class Darcy(torch.nn.Module):\n",
    "    \"Custom Darcy PDE definition for PINO\"\n",
    "\n",
    "    def __init__(self, gradient_method: str = \"hybrid\"):\n",
    "        super().__init__()\n",
    "        self.gradient_method = str(gradient_method)\n",
    "\n",
    "    def forward(self, input_var: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # get inputs\n",
    "        u = input_var[\"sol\"]\n",
    "        c = input_var[\"coeff\"]\n",
    "        dcdx = input_var[\"Kcoeff_y\"]  # data is reversed\n",
    "        dcdy = input_var[\"Kcoeff_x\"]\n",
    "\n",
    "        dxf = 1.0 / u.shape[-2]\n",
    "        dyf = 1.0 / u.shape[-1]\n",
    "        # Compute gradients based on method\n",
    "        # Exact first order and FDM second order\n",
    "        if self.gradient_method == \"hybrid\":\n",
    "            dudx_exact = input_var[\"sol__x\"]\n",
    "            dudy_exact = input_var[\"sol__y\"]\n",
    "            dduddx_fdm = ddx(\n",
    "                u, dx=dxf, channel=0, dim=0, order=1, padding=\"replication\"\n",
    "            )\n",
    "            dduddy_fdm = ddx(\n",
    "                u, dx=dyf, channel=0, dim=1, order=1, padding=\"replication\"\n",
    "            )\n",
    "            # compute darcy equation\n",
    "            darcy = (\n",
    "                1.0\n",
    "                + (dcdx * dudx_exact)\n",
    "                + (c * dduddx_fdm)\n",
    "                + (dcdy * dudy_exact)\n",
    "                + (c * dduddy_fdm)\n",
    "            )\n",
    "        # FDM gradients\n",
    "        elif self.gradient_method == \"fdm\":\n",
    "            dudx_fdm = dx(u, dx=dxf, channel=0, dim=0, order=1, padding=\"replication\")\n",
    "            dudy_fdm = dx(u, dx=dyf, channel=0, dim=1, order=1, padding=\"replication\")\n",
    "            dduddx_fdm = ddx(\n",
    "                u, dx=dxf, channel=0, dim=0, order=1, padding=\"replication\"\n",
    "            )\n",
    "            dduddy_fdm = ddx(\n",
    "                u, dx=dyf, channel=0, dim=1, order=1, padding=\"replication\"\n",
    "            )\n",
    "            # compute darcy equation\n",
    "            darcy = (\n",
    "                1.0\n",
    "                + (dcdx * dudx_fdm)\n",
    "                + (c * dduddx_fdm)\n",
    "                + (dcdy * dudy_fdm)\n",
    "                + (c * dduddy_fdm)\n",
    "            )\n",
    "        # Fourier derivative\n",
    "        elif self.gradient_method == \"fourier\":\n",
    "            dim_u_x = u.shape[2]\n",
    "            dim_u_y = u.shape[3]\n",
    "            u = F.pad(\n",
    "                u, (0, dim_u_y - 1, 0, dim_u_x - 1), mode=\"reflect\"\n",
    "            )  # Constant seems to give best results\n",
    "            f_du, f_ddu = fourier_derivatives(u, [2.0, 2.0])\n",
    "            dudx_fourier = f_du[:, 0:1, :dim_u_x, :dim_u_y]\n",
    "            dudy_fourier = f_du[:, 1:2, :dim_u_x, :dim_u_y]\n",
    "            dduddx_fourier = f_ddu[:, 0:1, :dim_u_x, :dim_u_y]\n",
    "            dduddy_fourier = f_ddu[:, 1:2, :dim_u_x, :dim_u_y]\n",
    "            # compute darcy equation\n",
    "            darcy = (\n",
    "                1.0\n",
    "                + (dcdx * dudx_fourier)\n",
    "                + (c * dduddx_fourier)\n",
    "                + (dcdy * dudy_fourier)\n",
    "                + (c * dduddy_fourier)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Derivative method {self.gradient_method} not supported.\")\n",
    "\n",
    "        # Zero outer boundary\n",
    "        darcy = F.pad(darcy[:, :, 2:-2, 2:-2], [2, 2, 2, 2], \"constant\", 0)\n",
    "        # Return darcy\n",
    "        output_var = {\n",
    "            \"darcy\": dxf * darcy,\n",
    "        }  # weight boundary loss higher\n",
    "        return output_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ac6ad",
   "metadata": {},
   "source": [
    "The gradients of the FNO solution are computed according to the gradient method selected above. The FNO model automatically outputs first order gradients when the `hybrid` method is used, and so no extra computation of these is necessary. Furthermore, note that the gradients of the permeability field are already included as tensors in the FNO input training data (with keys `Kcoeff_x` and `Kcoeff_y`) and so these do not need to be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be548d",
   "metadata": {},
   "source": [
    "### Load the Data and Initialize the Model \n",
    "\n",
    "The data loading follows a similar process as the FNO example. Only for the case where hybrid gradient method is used, you need to additionally instruct the model to output the appropriate gradients by specifying these gradients in its output keys.\n",
    "\n",
    "We will incorporate the `Darcy` model we defined earlier into Modulus by wrapping it into a Modulus `Node`. This ensures the module is incorporated into Modulus’ computational graph and can be used to optimize the FNO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e17bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from modulus.dataset import DictGridDataset\n",
    "from modulus.node import Node\n",
    "\n",
    "# load training/ test data\n",
    "input_keys = [\n",
    "    Key(\"coeff\", scale=(7.48360e00, 4.49996e00)),\n",
    "    Key(\"Kcoeff_x\"),\n",
    "    Key(\"Kcoeff_y\"),\n",
    "]\n",
    "output_keys = [\n",
    "    Key(\"sol\", scale=(5.74634e-03, 3.88433e-03)),\n",
    "]\n",
    "\n",
    "invar_train, outvar_train = load_FNO_dataset(\n",
    "    \"datasets/Darcy_241/piececonst_r241_N1024_smooth1.hdf5\",\n",
    "    [k.name for k in input_keys],\n",
    "    [k.name for k in output_keys],\n",
    "    n_examples=cfg.custom.ntrain,\n",
    ")\n",
    "invar_test, outvar_test = load_FNO_dataset(\n",
    "    \"datasets/Darcy_241/piececonst_r241_N1024_smooth2.hdf5\",\n",
    "    [k.name for k in input_keys],\n",
    "    [k.name for k in output_keys],\n",
    "    n_examples=cfg.custom.ntest,\n",
    ")\n",
    "\n",
    "# Define FNO model\n",
    "if cfg.custom.gradient_method == \"hybrid\":\n",
    "    output_keys += [\n",
    "        Key(\"sol\", derivatives=[Key(\"x\")]),\n",
    "        Key(\"sol\", derivatives=[Key(\"y\")]),\n",
    "    ]\n",
    "model = instantiate_arch(\n",
    "    input_keys=[input_keys[0]],\n",
    "    output_keys=output_keys,\n",
    "    cfg=cfg.arch.fno,\n",
    "    domain_length=[1.0, 1.0],\n",
    ")\n",
    "\n",
    "# Make custom Darcy residual node for PINO\n",
    "inputs = [\n",
    "    \"sol\",\n",
    "    \"coeff\",\n",
    "    \"Kcoeff_x\",\n",
    "    \"Kcoeff_y\",\n",
    "]\n",
    "if cfg.custom.gradient_method == \"hybrid\":\n",
    "    inputs += [\n",
    "        \"sol__x\",\n",
    "        \"sol__y\",\n",
    "    ]\n",
    "darcy_node = Node(\n",
    "    inputs=inputs,\n",
    "    outputs=[\"darcy\"],\n",
    "    evaluate=Darcy(gradient_method=cfg.custom.gradient_method),\n",
    "    name=\"Darcy Node\",\n",
    ")\n",
    "nodes = model.make_nodes(name=\"FNO\", jit=False) + [darcy_node]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620657e0",
   "metadata": {},
   "source": [
    "### Initializing the Domain, Adding Constraints and Validators\n",
    "\n",
    "Finally, we can add constraints to our model similarly to the FNO example. The same `SupervisedGridConstraint` can be used to include the PDE loss term we need to define additional target values for the `darcy` output variable defined above (zeros, to minimize the PDE residual) and add them to the `outvar_train` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0cc5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make domain\n",
    "domain = Domain()\n",
    "\n",
    "# add additional constraining values for darcy variable\n",
    "outvar_train[\"darcy\"] = np.zeros_like(outvar_train[\"sol\"])\n",
    "\n",
    "train_dataset = DictGridDataset(invar_train, outvar_train)\n",
    "test_dataset = DictGridDataset(invar_test, outvar_test)\n",
    "\n",
    "# add constraints to domain\n",
    "supervised = SupervisedGridConstraint(\n",
    "    nodes=nodes,\n",
    "    dataset=train_dataset,\n",
    "    batch_size=cfg.batch_size.grid,\n",
    ")\n",
    "domain.add_constraint(supervised, \"supervised\")\n",
    "\n",
    "# add validator\n",
    "val = GridValidator(\n",
    "    nodes,\n",
    "    dataset=test_dataset,\n",
    "    batch_size=cfg.batch_size.validation,\n",
    "    plotter=GridValidatorPlotter(n_examples=5),\n",
    "    requires_grad=True,\n",
    ")\n",
    "domain.add_validator(val, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd31243",
   "metadata": {},
   "source": [
    "### Solver and Training the Model\n",
    "\n",
    "Once the domain and the configuration is set up, the `Solver` can be defined, and the training can be started as seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make solver\n",
    "slv = Solver(cfg, domain)\n",
    "\n",
    "# start solver\n",
    "slv.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb9aae",
   "metadata": {},
   "source": [
    "### Results and Post-processing\n",
    "\n",
    "The network directory folder contains several plots of different validation predictions. One of them is shown below.\n",
    "\n",
    "<figure>\n",
    "    <figcaption>PINO validation predictions. (Left to right) Input permeability and its spatial derivatives, true pressure, predicted pressure, error.</figcaption>\n",
    "    <img src=\"pino_darcy_pred.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3a885",
   "metadata": {},
   "source": [
    "### Comparison to FNO\n",
    "\n",
    "The Tensorboard plot below compares the validation loss of PINO (all three gradient methods) and FNO. You can see that with large amounts of training data (1000 training examples), both FNO and PINO perform similarly. \n",
    "\n",
    "<img src=\"pino_darcy_tensorboard1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "A benefit of PINO is that the PDE loss regularizes the model, meaning that it can be more efficient in \"small data\" regimes. The plot below shows the validation loss when both models are trained with only 100 training examples. In this case, we find that PINO outperforms FNO. \n",
    "\n",
    "<img src=\"pino_darcy_tensorboard2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e049c",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867c0df",
   "metadata": {},
   "source": [
    "In the final section of this course you will perform a small exercise to solve a fluid mechanics problem involving the solution to the Navier-Stokes equations by using the PINN approach.\n",
    "\n",
    "Please continue to [the next notebook](../chip_2d/Exercise_CFD_Problem_Notebook.ipynb)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
