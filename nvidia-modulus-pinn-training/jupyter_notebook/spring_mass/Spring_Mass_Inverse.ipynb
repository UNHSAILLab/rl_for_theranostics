{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Inverse Spring Mass Problem\n",
    "\n",
    "Another important advantage of a neural network solver over traditional numerical methods is its ability to solve inverse problems. In an inverse problem, we start with a set of observations and then use those observations to calculate the causal factors that produced them. To demonstrate this concept we will solve the same spring mass system in an inverse setting. Specifically, we will assume that the data for the displacements of the 3 masses $x_1, x_2 \\text{ and } x_3$ is available to us through an analytical solution and the first mass ($m_1$) and the last spring coefficient ($k_4$) is unknown. We will use the neural network to assimilate the displacements from the analytical solution and use it to invert out the unknown quantities from ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Setup\n",
    "\n",
    "The equation file again remains the same for this part. Here, we will use the [`PointwiseConstraint`](https://docs.nvidia.com/deeplearning/modulus/api/modulus.domain.constraint.html#modulus.domain.constraint.continuous.PointwiseConstraint) class to assimilate the solution in the training data. We will make a network to memorize the three displacements by developing a mapping between $t$ and $(x_1, x_2, x_3)$. The second network will be trained to invert out the desired quantities viz. $(m_1, k_4)$.\n",
    "\n",
    "### Note\n",
    "\n",
    "Now we will describe the contents of the [`spring_mass_inverse.py`](../../source_code/spring_mass/spring_mass_inverse.py) script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Equations, Networks and Nodes for an Inverse Problem\n",
    "\n",
    "Since part of the problem involves memorizing the given solution, we will have `'t'` as an input NumPy array and `'x_1', 'x_2', 'x_3', 'ode_x1', 'ode_x2', 'ode_x3'` as the output NumPy arrays. Setting `'x_1', 'x_2', 'x_3'` as input values from analytical data, we are essentially making the network assimilate the analytical distribution of these variables in the selected domain. Setting `'ode_x1', 'ode_x2', 'ode_x3'` equal to 0, we will also inform the network to satisfy the ODE losses at those sampled points. Now, assuming that we also know the initial conditions of the solution, except the `'m_1'` and `'k_4'`, all the variables in these ODEs are known. Thus, the network can use this information to invert out the unknowns. The below code shows the data preparation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from sympy import Symbol, Eq\n",
    "\n",
    "import modulus\n",
    "from modulus.hydra import ModulusConfig, instantiate_arch\n",
    "from modulus.solver import Solver\n",
    "from modulus.domain import Domain\n",
    "from modulus.geometry.primitives_1d import Point1D\n",
    "from modulus.geometry import Parameterization\n",
    "from modulus.domain.constraint import (\n",
    "    PointwiseBoundaryConstraint,\n",
    "    PointwiseConstraint,\n",
    ")\n",
    "from modulus.domain.validator import PointwiseValidator\n",
    "from modulus.domain.monitor import PointwiseMonitor\n",
    "from modulus.key import Key\n",
    "from modulus.node import Node\n",
    "\n",
    "from spring_mass_ode import SpringMass\n",
    "\n",
    "@modulus.main(config_path=\"conf\", config_name=\"config_inverse\")\n",
    "def run(cfg: ModulusConfig) -> None:\n",
    "    # prepare data\n",
    "    t_max = 10.0\n",
    "    deltaT = 0.01\n",
    "    t = np.arange(0, t_max, deltaT)\n",
    "    t = np.expand_dims(t, axis=-1)\n",
    "\n",
    "    invar_numpy = {\"t\": t}\n",
    "    outvar_numpy = {\n",
    "        \"x1\": (1 / 6) * np.cos(t)\n",
    "        + (1 / 2) * np.cos(np.sqrt(3) * t)\n",
    "        + (1 / 3) * np.cos(2 * t),\n",
    "        \"x2\": (2 / 6) * np.cos(t)\n",
    "        + (0 / 2) * np.cos(np.sqrt(3) * t)\n",
    "        - (1 / 3) * np.cos(2 * t),\n",
    "        \"x3\": (1 / 6) * np.cos(t)\n",
    "        - (1 / 2) * np.cos(np.sqrt(3) * t)\n",
    "        + (1 / 3) * np.cos(2 * t),\n",
    "    }\n",
    "    outvar_numpy.update({\"ode_x1\": np.full_like(invar_numpy[\"t\"], 0)})\n",
    "    outvar_numpy.update({\"ode_x2\": np.full_like(invar_numpy[\"t\"], 0)})\n",
    "    outvar_numpy.update({\"ode_x3\": np.full_like(invar_numpy[\"t\"], 0)})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating neural networks for an inverse problem is similar to other problems we have seen in previous sections. However, as the information for the displacements, and in turn their gradients (i.e. velocities and accelerations), is already present (from analytical data) for the network to memorize, we will detach these variables and computation graph in their respective equations. This means that only the networks predicting `'m1'` and `'k4'` will be optimized to minimize the equation residuals. The displacements, velocities, and accelerations are treated as ground truth data.\n",
    "\n",
    "Also, note that the mass and spring constant are passed in as Symbolic variables (`'m1'` and `'k4'` respectively) to the\n",
    "equations as they are unknowns in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # make list of nodes to unroll graph on\n",
    "    sm = SpringMass(k=(2, 1, 1, \"k4\"), m=(\"m1\", 1, 1))\n",
    "    sm_net = instantiate_arch(\n",
    "        input_keys=[Key(\"t\")],\n",
    "        output_keys=[Key(\"x1\"), Key(\"x2\"), Key(\"x3\")],\n",
    "        cfg=cfg.arch.fully_connected,\n",
    "    )\n",
    "    invert_net = instantiate_arch(\n",
    "        input_keys=[Key(\"t\")],\n",
    "        output_keys=[Key(\"m1\"), Key(\"k4\")],\n",
    "        cfg=cfg.arch.fully_connected,\n",
    "    )\n",
    "\n",
    "    nodes = (\n",
    "        sm.make_nodes(\n",
    "            detach_names=[\n",
    "                \"x1\",\n",
    "                \"x1__t\",\n",
    "                \"x1__t__t\",\n",
    "                \"x2\",\n",
    "                \"x2__t\",\n",
    "                \"x2__t__t\",\n",
    "                \"x3\",\n",
    "                \"x3__t\",\n",
    "                \"x3__t__t\",\n",
    "            ]\n",
    "        )\n",
    "        + [sm_net.make_node(name=\"spring_mass_network\", jit=cfg.jit)]\n",
    "        + [invert_net.make_node(name=\"invert_network\", jit=cfg.jit)]\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we will use the solution coming from the analytical solution in the form of the NumPy arrays we defined earlier. We will use the `PointwiseConstraint` class to handle such input data. The `PointwiseConstraint` class takes in separate dictionaries for input variables and output variables. These dictionaries have a key for each variable and a NumPy array of values associated to the key. The below code shows the addition of the boundary constraint and the constraint coming from the NumPy arrays we just defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # add constraints to solver\n",
    "    # make geometry\n",
    "    geo = Point1D(0)\n",
    "    t_symbol = Symbol(\"t\")\n",
    "    x = Symbol(\"x\")\n",
    "    time_range = {t_symbol: (0, t_max)}\n",
    "\n",
    "    # make domain\n",
    "    domain = Domain()\n",
    "\n",
    "    # initial conditions\n",
    "    IC = PointwiseBoundaryConstraint(\n",
    "        nodes=nodes,\n",
    "        geometry=geo,\n",
    "        outvar={\"x1\": 1.0, \"x2\": 0, \"x3\": 0, \"x1__t\": 0, \"x2__t\": 0, \"x3__t\": 0},\n",
    "        batch_size=cfg.batch_size.IC,\n",
    "        lambda_weighting={\n",
    "            \"x1\": 1.0,\n",
    "            \"x2\": 1.0,\n",
    "            \"x3\": 1.0,\n",
    "            \"x1__t\": 1.0,\n",
    "            \"x2__t\": 1.0,\n",
    "            \"x3__t\": 1.0,\n",
    "        },\n",
    "        parameterization=Parameterization({t_symbol: 0}),\n",
    "    )\n",
    "    domain.add_constraint(IC, name=\"IC\")\n",
    "\n",
    "    # data and pdes\n",
    "    data = PointwiseConstraint.from_numpy(\n",
    "        nodes=nodes,\n",
    "        invar=invar_numpy,\n",
    "        outvar=outvar_numpy,\n",
    "        batch_size=cfg.batch_size.data,\n",
    "    )\n",
    "    domain.add_constraint(data, name=\"Data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Monitors\n",
    "\n",
    "For this section, we will monitor the convergence of average `'m_1'` and `'k_4'` inside the domain as the solution progresses. Once we find that the average value of these quantities have reached a steady value, we can end the simulation. The code to generate such monitors can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # add monitors\n",
    "    monitor = PointwiseMonitor(\n",
    "        invar_numpy,\n",
    "        output_names=[\"m1\"],\n",
    "        metrics={\"mean_m1\": lambda var: torch.mean(var[\"m1\"])},\n",
    "        nodes=nodes,\n",
    "    )\n",
    "    domain.add_monitor(monitor)\n",
    "\n",
    "    monitor = PointwiseMonitor(\n",
    "        invar_numpy,\n",
    "        output_names=[\"k4\"],\n",
    "        metrics={\"mean_k4\": lambda var: torch.mean(var[\"k4\"])},\n",
    "        nodes=nodes,\n",
    "    )\n",
    "    domain.add_monitor(monitor)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver and Training\n",
    "\n",
    "Now that we have the definitions for the various constraints and domains complete, we can form the solver and run the problem similar to before. The code to do the same can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # make solver\n",
    "    slv = Solver(cfg, domain)\n",
    "\n",
    "    # start solver\n",
    "    slv.solve()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/spring_mass_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/spring_mass/spring_mass_inverse.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Solution\n",
    "\n",
    "We can monitor the Tensorboard plots to see the convergence of the simulation and the final values of the $m_1$ and $k_4$. We see that after sufficient training, we get the $m_1$ and $k_4$ as 1 and 2 respectively. These were the same values that we used to create the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we will cover some theory behind the Fourier Neural Operator (FNO) and some of its variants. We will then apply these architectures on a Darcy flow field prediction problem in a data-driven fashion.\n",
    "\n",
    "Please continue to [the next notebook](../Operators/FNO_Darcy.ipynb)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
